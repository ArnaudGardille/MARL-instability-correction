# --- Defaults ---

seed: 0
torch_deterministic: True
cuda: False
track: False
display_video: False
capture_video: False
save_model: True
use_state: False
save_buffer: False
load_buffer: False
save_imgs: False
load_agents_from: 
load_buffer_from: 
    

    # Environment specific arguments
n_agents: 10
n_actions: 2

    # Algorithm specific arguments
env_id: simultaneous
random_policy: False
no_training: False
total_timesteps: 10000
learning_rate: 0.01
buffer_size: 10000
gamma: 0.99
tau: 1.
evaluation_frequency: 10
evaluation_episodes: 1
target_network_frequency: 50
batch_size:  32 #2**18, #256, #
start_e: 0.99
end_e: 0.05
exploration_fraction: 0.8
learning_starts: 100
train_frequency: 10
single_agent: False
add_id: False
add_epsilon: False
add_others_explo: False
dueling: False
deterministic_env: False
boltzmann_policy: False
loss_corrected_for_others: False
loss_not_corrected_for_priorisation: False
prio: td_error #choices=['td_error', 'td-past', 'td-cur-past', 'td-cur', 'cur-past', 'cur'], default='')
rb: uniform # choices=['uniform', 'prioritized', 'laber'], default='uniform',

loss_correction_for_others: 'none'
sqrt_correction: False
clip_correction_after: 